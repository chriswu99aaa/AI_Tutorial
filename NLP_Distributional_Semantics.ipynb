{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chriswu99aaa/AI_Tutorial/blob/master/NLP_Distributional_Semantics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_e7hthJtM31"
      },
      "source": [
        "Pre-processing need to be done before constructing the model\n",
        "1. Tokenization\n",
        "2. Lower casing\n",
        "3. Stop words removal\n",
        "4. Stemming\n",
        "5. Lemmatization\n",
        "\n",
        "The overall procedure for task1 (1) is\n",
        "1. Load the training data\n",
        "2. Pre-processing\n",
        "  * process multi word representation: models.phrases in gensim\n",
        "3. Build the tf-idf representation by ourself or using NLTK API\n",
        "\n",
        "Evaluate the solution by constructing a function which retunrs the cosine similarity.\n",
        "Build the similarity function\n",
        "\n",
        "Further action, check similarity for multiple words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtjpzeKXx5Sk",
        "outputId": "073d9713-d392-4687-a4c1-8084ee70ae92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf2ovWMZ09If"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import Text\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim import corpora, models, similarities\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeA3ugEJ1p85",
        "outputId": "72b8baad-8e83-4a4e-deae-52d235f96bc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Connect to google drive since the training data is store in it\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Access files in Google Drive\n",
        "file_path = '/content/drive/MyDrive/NLP/Training-dataset.csv'\n",
        "\n",
        "\n",
        "def file_loader(file_path):\n",
        "    return pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVnbw4C-qmmw"
      },
      "source": [
        "Better search results: Lemmatization helps in retrieving better search results since it reduces different forms of a word to a common base form, making it easier to match different forms of a word in the text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaMwsM7Epm1G"
      },
      "source": [
        "Whether to use stemming or lemmatization or both needs to be tested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeMkpnpg4Cj0"
      },
      "outputs": [],
      "source": [
        "def pre_processing(df):\n",
        "    '''\n",
        "    perform pre-processing to the plot synopsis column\n",
        "    '''\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "\n",
        "    # Convert the words into lower case\n",
        "    df['plot_synopsis'] = df['plot_synopsis'].str.lower()\n",
        "\n",
        "    # Tokenize sentences\n",
        "    df['plot_synopsis'] = df['plot_synopsis'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "    # Remove stopwords and other non-alphabetic character\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    df['plot_synopsis'] = df['plot_synopsis'].apply(lambda x:[word for word in x if word.isalpha() and word not in stop_words])\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    df['plot_synopsis'] = df['plot_synopsis'].apply(lambda x:[lemmatizer.lemmatize(word) for word in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDQbMI_h_1-6"
      },
      "outputs": [],
      "source": [
        "def identify_phrase(df):\n",
        "    '''\n",
        "    This function uses the gensim Phrase class which handles multi word representation\n",
        "    '''\n",
        "    from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
        "\n",
        "    bigram_model = Phrases(df['plot_synopsis'], min_count=1, threshold=1,connector_words=ENGLISH_CONNECTOR_WORDS)\n",
        "    trigram_model = Phrases(bigram_model[df['plot_synopsis']], min_count=1, threshold=1, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
        "    df['plot_synopsis'] = df['plot_synopsis'].apply(lambda x: trigram_model[x])\n",
        "\n",
        "def build_dict(df, mode = 0):\n",
        "    '''\n",
        "    Build the dictionary representation from text\n",
        "    '''\n",
        "\n",
        "    # Construct dictionary using\n",
        "    if mode == 1:\n",
        "        dictionary = corpora.Dictionary(df['plot_synopsis'])\n",
        "\n",
        "        # Apply the function docwbow to convert each entry in the plot_synopsis column into BOW representation\n",
        "        df['bow'] = df['plot_synopsis'].apply(dictionary.doc2bow)\n",
        "    else:\n",
        "        dictionary = corpora.Dictionary(df.iloc[:,0])\n",
        "\n",
        "    return dictionary\n",
        "\n",
        "def build_corpus(dictionary, df):\n",
        "    # Construct a corpus as a list\n",
        "    return [dictionary.doc2bow(doc) for doc in df['plot_synopsis']]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO3qxbZiTFQ2"
      },
      "outputs": [],
      "source": [
        "def tf_idf(corpus):\n",
        "    '''\n",
        "    Build the TF-IDF model\n",
        "    '''\n",
        "    # Construct the TF-IDF Model\n",
        "    return models.TfidfModel(corpus)\n",
        "\n",
        "    # # Transform corpus to TF-IDF representation\n",
        "    # tfidf_corpus = tf_idf[corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0OCT8TZSIyI",
        "outputId": "2c8c587a-bb55-4720-8390-1a32179d152b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "df = file_loader(file_path)\n",
        "\n",
        "# pre-processing\n",
        "pre_processing(df)\n",
        "\n",
        "# use phrase to handle multiword representation\n",
        "identify_phrase(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwBTJluvzuBF"
      },
      "outputs": [],
      "source": [
        "document = df['plot_synopsis'].values.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRdJUvdZ0UBQ"
      },
      "outputs": [],
      "source": [
        "dictionary = corpora.Dictionary(document)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary.token2id"
      ],
      "metadata": {
        "id": "uGWNjUu_QtE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [dictionary.doc2bow(tokens) for tokens in document]\n"
      ],
      "metadata": {
        "id": "O0_0n0aM0II4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fORub4Hp2kyf"
      },
      "outputs": [],
      "source": [
        "model = models.TfidfModel(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvLqGFM_NIm7"
      },
      "outputs": [],
      "source": [
        "validation_df = pd.read_csv('/content/drive/MyDrive/NLP/Task-1-validation-dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5rwSyQ8MDRI"
      },
      "outputs": [],
      "source": [
        "words1 = validation_df.iloc[:, [1]]\n",
        "words2 = validation_df.iloc[:, [2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW-NPzds_fU5"
      },
      "outputs": [],
      "source": [
        "words1 = words1.values.tolist()\n",
        "words2 = words2.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words1_tfidf_list = []\n",
        "words2_tfidf_list = []"
      ],
      "metadata": {
        "id": "Rw2P32Gg452J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yNeaLc9_74G"
      },
      "outputs": [],
      "source": [
        "for i in range(len(words1)):\n",
        "    word1_bow = dictionary.doc2bow(words1[i])\n",
        "    word2_bow = dictionary.doc2bow(words2[i])\n",
        "\n",
        "    word1_tfidf = model[word1_bow]\n",
        "    word2_tfidf = model[word2_bow]\n",
        "\n",
        "    words1_tfidf_list.append(word1_tfidf)\n",
        "    words2_tfidf_list.append(word2_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(words1)):\n",
        "    if len(words1_tfidf_list[i])==0:\n",
        "        words1_tfidf_list[i].append((0,0))\n",
        "        print('OOV index: ',i)\n",
        "    if len(words2_tfidf_list[i])==0:\n",
        "        words2_tfidf_list[i].append((0,0))\n",
        "        print('OOV index: ',i)"
      ],
      "metadata": {
        "id": "5NCxkCNy6pWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX1CqdZgJxwv"
      },
      "outputs": [],
      "source": [
        "for i in range(len(words1_tfidf_list)):\n",
        "    print(i)\n",
        "    w1 = words1_tfidf_list[i]\n",
        "    w2 = words2_tfidf_list[i]\n",
        "    print('w1: ',w1)\n",
        "    print('w2: ',w2)\n",
        "    sim = cosine_similarity(w1,w2)\n",
        "    prediction.append(sim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for i in range(len(prediction)):\n",
        "    result.append(prediction[i][0][0])\n",
        "\n",
        "# save the result to result.csv\n",
        "results = pd.DataFrame(result)\n",
        "path = \"/content/drive/MyDrive/NLP/result.csv\"\n",
        "results.to_csv(path,index=False)"
      ],
      "metadata": {
        "id": "bjKG2nChBAjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(result)\n",
        "\n",
        "path = \"/content/drive/MyDrive/NLP/result1.csv\"\n",
        "results.to_csv(path,index=False)"
      ],
      "metadata": {
        "id": "fan_XaCmRYRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UyrScPUXktO"
      },
      "outputs": [],
      "source": [
        "# import gensim.downloader as api\n",
        "# from gensim.models import TfidfModel\n",
        "# from gensim.corpora import Dictionary\n",
        "\n",
        "# dataset = api.load(\"text8\")\n",
        "# dct = Dictionary(dataset)  # fit dictionary\n",
        "# corpus = [dct.doc2bow(line) for line in dataset]  # convert corpus to BoW format\n",
        "\n",
        "# model = TfidfModel(corpus)  # fit model\n",
        "# vector = model[corpus[0]]  # apply model to the first corpus document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJizLb5Bs1ZR"
      },
      "source": [
        "## Task 1(2) Word2vec\n",
        "\n",
        "Bulding up from the pre-processing result, we are going to use word2vec from gensim."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "df = file_loader(file_path)\n",
        "\n",
        "# pre-processing\n",
        "pre_processing(df)\n",
        "\n",
        "# use phrase to handle\n",
        "identify_phrase(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a4xla_1S8wR",
        "outputId": "5bda0952-6783-43af-d8be-df6d33d7074c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = df['plot_synopsis'].values.tolist()"
      ],
      "metadata": {
        "id": "tqBA5wpYULjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt_dkuIVtAIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c3e40f-bea7-4a47-c2d2-bcb7ec99a735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26998530, 26998530)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "# Create Word2Vec model\n",
        "word_vec_model = models.Word2Vec(sentences=document, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Train the Word2Vec model\n",
        "word_vec_model.train(df['plot_synopsis'], total_examples=len(df['plot_synopsis']), epochs=10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vec_model.wv['apple'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19-f9W2_UvVA",
        "outputId": "95bf48da-18d0-409b-b5c4-272df3f35b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeurh_szv3ns"
      },
      "outputs": [],
      "source": [
        "validation_df = pd.read_csv('/content/drive/MyDrive/NLP/Task-1-validation-dataset.csv')\n",
        "\n",
        "words1 = validation_df.iloc[:, [1]]\n",
        "words2 = validation_df.iloc[:, [2]]\n",
        "\n",
        "words1 = words1.values.tolist()\n",
        "words2 = words2.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words2[63]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxQMVSdNaSzu",
        "outputId": "41f2c211-197f-44ed-bc4d-f0c48506f930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tableware']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.zeros((100,)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UsIJb0ygPTh",
        "outputId": "055cb0aa-2428-4f3d-bca6-b0358227489a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.rand(100,).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLv-E7hmgPOg",
        "outputId": "9cb12e80-8cd2-454a-cfb3-04d80316db96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words1:\n",
        "    if word in word_vec_model.wv:\n"
      ],
      "metadata": {
        "id": "HZoyW3mQfd-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ3beVExQayh"
      },
      "outputs": [],
      "source": [
        "sim = []\n",
        "# try:\n",
        "for i in range(len(words1)):\n",
        "    if (words1[i][0] in word_vec_model.wv) and (words2[i][0] in word_vec_model.wv):\n",
        "        s = word_vec_model.wv.similarity(words1[i][0], words2[i][0])\n",
        "        sim.append(s)\n",
        "    else:\n",
        "        w1 = np.random.rand(100,)\n",
        "        w2 = np.random.rand(100,)\n",
        "        s = np.dot(w1,w2)/(np.linalg.norm(w1) * np.linalg.norm(w2))\n",
        "        sim.append(s)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the result to result.csv\n",
        "results2 = pd.DataFrame(sim)\n",
        "path = \"/content/drive/MyDrive/NLP/result2.csv\"\n",
        "results2.to_csv(path,index=False)"
      ],
      "metadata": {
        "id": "uCXn_PvuihSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2wxNAeGP_MF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1ea677-2628-489a-8056-e6b7d9141e06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.80618453"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "word_vec_model.wv.similarity('absorb','study')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaZXOyVQvA9l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvzUG/JtRogDZCx/dUojXU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}